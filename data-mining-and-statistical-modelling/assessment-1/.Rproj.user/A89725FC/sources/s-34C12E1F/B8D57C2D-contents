---
title: "MS4S09 Coursework 1 - 2020/21"
subtitle: "17076749"
author: "Mark Baber"
date: "06/01/2021"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE)
```
# 1 - Introduction

This report will be my assignment for MS4S09 where the exploration of data mining, sentiment analysis and topic modelling will be used to analyse a dataset for an airline.

Before anything, lets put any of the packages used to carry out the analysis below.

```{r install packages, message=FALSE, warning=FALSE, paged.print=FALSE}
# install.packages(c("dplyr","ggplot2","RColorBrewer","SnowballC","textdata","tidyr","tidytext","tm","topicmodels","wordcloud"))
```

Now load the packages which will be used.

```{r load packages, message=FALSE, warning=FALSE, paged.print=FALSE}
library(dplyr)
library(ggplot2) 
library(RColorBrewer) 
library(reshape2)
library(SnowballC) 
library(textdata)
library(tidyr)
library(tidytext)
library(tm) 
library(topicmodels)
library(wordcloud)
```

Start by reading in the data.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#Read in the data
Data <- read.csv("airline_coursework(1).csv", stringsAsFactors = FALSE)
```

## Get a better picture

It is good practice to understand the data, this can be done by looking at a summary, head, class and structure of the data.
of the data

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
#summary(Data)
Data %>% 
  head(n = 5)
#class(Data)
#str(Data)
```

Most of this dataset is made up of characters which can't really be plotted yet. 
So lets instead look at the different types of airlines within the dataset.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# check unique as unlist.
Data$airline %>% 
  unlist() %>% 
  unique()
# count the na values
Data %>% 
  is.na() %>% 
  sum()
# drop na values
Data <- drop_na(Data) # drop na values
# count again as sanity check.
Data %>% 
  is.na() %>% 
  sum()
# list unique again.
Data$airline %>% 
  unlist() %>% 
  unique()
```

Within the dataset there were 3 airlines, albeit one is empty. So above the empty dataset was found and removed from the dataset with a built in function called drop_na. After removing the na value, the unique airlines are then counted again as a sanity check.

# 2 - Text Mining
To ensure the next few steps are easier, lets copy the dataframe into a corpus, which will allow for manipulating with the function getTransformations() which is from the tm package..

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# create a corpus of the text col
dcText <- Corpus(VectorSource(Data$text))
getTransformations()
```

Next lets look at cleaning up the data a bit by:
1 - changing it to lowercase
2 - remove numbers
3 - remove punctuation
4 - remove common stop words
5 - stem the document has been left out due to changing united to unit
6 - strip white space

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
# change the content to lowercase
dcText = tm_map(dcText, content_transformer(tolower)) 
# remove numbers
dcText = tm_map(dcText, removeNumbers)
# remove punctuation
dcText = tm_map(dcText, removePunctuation)
# remove stopwords from the english dictionary
dcText = tm_map(dcText, removeWords, (stopwords("english")))
# dcText = tm_map(dcText, stemDocument)
dcText = tm_map(dcText, stripWhitespace)
```

Now to create a function to transform the text, this will be a custom function which we learnt from lecture. This function toSpace() will transform/manipulate the content which is given to the function, and replace the content within ' " " ' to a blank space. 

- example "/" will change to " "

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
toSpace <- 
  content_transformer(function (x , pattern ) gsub(pattern, " ", x))

tText <- tm_map(dcText, toSpace, "/")
tText <- tm_map(dcText, toSpace, "@")
tText <- tm_map(dcText, toSpace, "\\|")
```

Next lets create a term document matrix which is a matrix of all the terms within the document (terms being words).

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
myDTM <- TermDocumentMatrix(tText)
m <- as.matrix(myDTM)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
```

Now lets plot these top ten words and have a look at them within a bar plot.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
theme_set(theme_minimal())
myFill = '#E7298A'
myColour = '#000000'

barplot(d[1:10,]$freq, 
        las = 2, 
        names.arg = d[1:10,]$word,
        col = myFill, main ="Top 10 words",
        xlab = "Frequency",
        ylab = "Word",
        horiz = TRUE)
```

Whilst showing this in a barplot can be good, it would look like some words would need to be pre-processed more when moving to the next step, sentiment analysis. The words such as get, can, just don't really add any value, but we can clearly see people are tweeting to united a lot more than virginamerica.

For a more interesting chart, lets look at making a wordcloud, including all of the words with the frequency representing the words size.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=100, random.order=F, rot.per=0.25, 
          colors=brewer.pal(4, "Dark2"))
```

Now that we have seen the top words and a lovely chart, lets see how it changes when removing the 2 companies names and flight from the dataset. As these 3 are very much skewing the charts.
```{r}
# create my custom words list
custom_stop_words <- bind_rows(tibble(word = c("united","flight","virginamerica"),lexicon = c("custom")), stop_words)

d <- d %>% # update d with the removal of new custom stop words
  anti_join(custom_stop_words)
```
In the section above, there was a tibble which contained 3 custom stopwords, which was then used to remove these stop words from the dataframe d.
Now to plot the dataset again, to see how it has been affected with the removal of the custom stopwords.

```{r}
barplot(d[1:10,]$freq, 
        las = 2, 
        names.arg = d[1:10,]$word,
        col = myFill, main ="Top 10 words",
        xlab = "Frequency",
        ylab = "Word",
        horiz = TRUE)
```
This new chart shows a more evenly distributed frequency for the words, with server being the top word. This could easily be good server, or bad service. 

Next lets take a look at the wordcloud again.

```{r message=FALSE, warning=FALSE, paged.print=FALSE}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
          max.words=100, random.order=F, rot.per=0.25, 
          colors=brewer.pal(4, "Dark2"))
```

This word cloud doesn't look at nice as the first one, but here we can see the words are similar in size which shows how much the custom stop words were throwing off the charts.

# 3 - Sentiment Analysis

This section will look at using Sentiment Analysis techniques to measure the sentiment per word/sentence within the dataset.

```{r}
# get a copy of the data for Sentiment Analysis section
saData <- Data 
saData %>% 
  head(n = 5)
```

Next will be to tokenize the texts column into a new column.

```{r}
saData <- saData %>% 
  unnest_tokens(word, text)

saData %>% 
  head(n = 5)
```

Now there is a new column called word which has our tokenized text. Next we will continue to remove the stopwords.

```{r}
saData <- saData %>% 
  anti_join(stop_words) # 27411

saData %>% 
  head(n = 5)
```

After removing the stop words, lets see what are the most popular words again.

```{r}
saData %>% 
  count(word, sort = TRUE) %>% 
  head(n=5)
```

Above we can see there 3 most popular words are united, flight and virginamerica. Whilst not surprising they could also be omitted to try and get a more evenly distributed dataset. To copy the steps above, lets remove the custom stop words and some other words which don't really make sense. ("t.co" ,"http"  "2")

```{r}
# update custom stop words
custom_stop_words <- 
  bind_rows(tibble(word = c("united","flight","virginamerica", "t.co","http", "2"),lexicon = c("custom")), stop_words)
# remove custom stop words pt 2
saData <- saData %>% 
  anti_join(custom_stop_words) # 27411
# sanity check to see how it looks now.
saData %>% 
  count(word, sort = TRUE) %>% 
  head(n=5)

```


this could also be plotted for a better visual using ggplot.

```{r}
saData %>%
  count(word, sort = TRUE) %>%
  filter(n > 125) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(word, n)) +
  geom_col(fill = myFill, color = myColour) +
  labs(title = "Highest recoccouring word",
       subtitle = "With custom stop words removed",
       y = "Observations", x = "Terms") +
  coord_flip()
```

The plot above shows the top ten most popular words with more than 125 observations, a lot of these words could go either way really. They could be positive or negative, so in the next steps lets look at investigating these words.

```{r}
cleanSA <- saData %>%
  anti_join(stop_words, by=c("word"="word"))
# count again for a sanity check
saData %>% 
  count(word, sort = TRUE) %>% 
  head(n = 5)
```

Now lets figure out the actual sentiment for the dataset.

There are many lexicon libraries which are out there, but for this report lets focus on 2 and maybe adding a third party library later.
These libraries are 'bing' and 'nrc', lets check the sentiment for the dataset using these 2 libraries.

## bing
```{r}
bingSA <- cleanSA %>%
  inner_join(get_sentiments("bing")) %>%
  count(word, sort = T)
bingSA %>% 
  head(n = 5)
```
The most popular words within our dataset against the bing library.

## nrc
```{r}
nrcSA <- cleanSA %>%
  inner_join(get_sentiments("nrc")) %>%
  count(word, sort = T)
nrcSA %>% 
  head(n = 5)
```
The most popular words within our dataset against the nrc library.

Lets plot both of these, to get a better visual of the differences.

```{r}
# create an index for cleanSA
cleanSA$ID <- seq.int(nrow(cleanSA))

# combine both of the dictionaries
bingNRC <- bind_rows(cleanSA %>% 
      inner_join(get_sentiments("bing")) %>%
        mutate(method = "Bing Sentiments."), # using the bing dictionary, combine all sentiments as bing sentiments
  cleanSA %>% 
      inner_join(get_sentiments("nrc") %>% 
        filter(sentiment %in% c("positive", "negative"))) %>%
        mutate(method = "NRC Sentiments")) %>% # using the nrc dictionary, combine all sentiments as nrc sentiments
            count(method, index=ID %% 150, sentiment) %>% # count all entries of both dictionaries, with neg, pos and overall sentiment.
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)
```

Now lets plot it to get an idea of how the sentiment is per 150 lines.

```{r}
bingNRC %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = F) +
  facet_wrap(~method, ncol = 1, scales = "free_x") +
  labs(title = "Comparing sentiments",
  subtitle = "Between the lexicon dictionaries, bing and nrc",
  y = "Emotion", x = "Observations")

```

Now this lovely negative graph shows us the overall sentiment total per 150 lines, bing measures some of the words a lot harsher, with what looks like 0 positives. Yet NRC shows not as much negative scores (but very close) with even at least 1 positive sentiment.

```{r}
# Bing
get_sentiments("bing") %>% 
  head(n = 5)
# nrc
get_sentiments("nrc") %>% 
  head(n = 5)
```

As mentioned above, there are also other lexicon dictionaries, some which are community driven and open source, with others being behind a pay wall.
Lets look at the dictionary afinn.

```{r}
# afinn
get_sentiments("afinn") %>% head(n = 5)
```

The good thing about afinn is that it ranks the words with a value, which for example, could be used to find the most negative episode of a tv show. 
For now lets use a similar method for afinn as we did for bing and nrc.

```{r}
afinnSA <- cleanSA %>%
  inner_join(get_sentiments('afinn')) %>%
  count(word, value, sort = TRUE)
afinnSA[1:10,]
```
This output shows the word which, the value of the word (This can range from -5 to +5 (Neg to Pos)) with the number of observations within the dataset.

```{r}
afinnSA %>%
  filter(n > 40) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word, color=word)) +
  geom_col(fill='#ffffff') +
  labs(title = "Highest recoccouring word",
       subtitle = "Within the lexicon dictionary, afinn",
       y = "Terms", x = "Observations")
```


# 4 - Topic Modelling
Lets start by getting a copy of our cleaned sentiment analysis data (to speed things up.)
```{r}
topicD <- cleanSA
topicD %>% 
  count(word, sort = T) %>% 
  head(n = 5)
```
Next the data frame would need to be changed to a Document Term Matrix

```{r}
# change to corpus using TM packages
myCorp <- Corpus(VectorSource(topicD$word))
# now convert to DocumentTermMatrix and clean it
myDTM <- DocumentTermMatrix(myCorp, control = list())
```

After converting to a corpus, then to a DTM, this dataset can now be used with the LDA model which is part of the topic models package.
Lets start by assuming there are 2 topics and see how the topics fit. This could also be changed later.

```{r}
# If the DTM has errors when creating an LDA,
# find all empty rows and remove them.
#sum by raw each raw of the table
raw.sum = apply(myDTM,1,FUN=sum)
# remove the 
myDTM = myDTM[raw.sum!=0,]

myLDA <- LDA(myDTM, k=2, control=list(seed=1234))
```
With the LDA created, the next steps would be to extract how the data falls into 2 topics.

```{r}
ldaTopics <- tidy(myLDA, matrix="beta")
ldaTopics
```
The output above shows each topic within the document, against the term/word with the beta which is the probability/% of accuracy.

```{r}
# get the top 10 terms from ldaTopics
topTerms <- ldaTopics %>%
  group_by(topic) %>%
  top_n(10) %>%
  ungroup() %>%
  mutate(term = reorder(term, beta)) %>%  # count by term, then data.
  arrange(topic, -beta)
# not plot the top terms
topTerms %>%
  ggplot(aes(term, beta, fill = factor(topic), colour=myColour)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~ topic, scales = "free") +
  labs(title = "Top ten words in each topic",
       y = "Beta", x = "Term")+
  coord_flip() 
```

The two topics now show the top 10 words for both topic 1 and topic 2. These words are the highest contributors to the weight of each topic.

With the top 3 words being different in both of these topics, we can assume it has worked and notice the difference. With topic 1 showing, time, cancelled and airline, and topic 2 showing, planes, flight, service.

Due to this being quite difficult to interpret, lets look at comparing the biggest differences in words, between all topics.

```{r}
beta_spread <- ldaTopics %>%
  mutate(topic = paste0("topic", topic)) %>%
  spread(topic, beta) %>%
  filter(topic1 > .001 | topic2 > .001) %>%
  mutate(log_ratio = log2(topic2 / topic1))

beta_spread
```
Comparing the two betas between both topics is a good way to see how much the words can vary from the two topics using the LDA Model.
This was done with a log2 ratio to make the differences more symmetrical with negative and positive values.

Whilst this shows a lot of information about the differences between the words and the topics, it is hard to visualise.
So the plot below will look at the top 10 words with the biggest differences.

```{r}
beta_top_terms <- beta_spread %>%
  top_n(10, log_ratio) 

beta_top_terms %>%
  ggplot(aes(term, log_ratio, fill="#E7298A")) +
  geom_col(show.legend = FALSE) +
  coord_flip() +
  labs(title = "Words with biggest differences in the 2 topics",
       y = "Terms", x = "Log Ratio")
```
These top ten words shows the words which have the biggest differences between the two topics, whilst all of the dataset is around airlines, by suggesting there is only two topics the LDA model has managed to split them as best as it could.

# 5 - Further Exploration

# 5.1 - Compare original Sentiment column
This section, lets have a look at the original sentiment column of the dataset.

```{r}
# get a copy of the data
ogSA <- Data
# create 2 data frames
ogPos <- data.frame()
ogNeg <- data.frame()
# filter and count neg
ogNeg <-
  ogSA %>% 
  filter(ogSA$sentiment == "negative") %>%
  count()
# add colnames
colnames(ogNeg)[colnames(ogNeg) == "n"] <- "original-negative"
# filter and count pos
ogPos <- 
  ogSA %>% 
    filter(ogSA$sentiment == "positive") %>% 
  count()
# add colnames
colnames(ogPos)[colnames(ogPos) == "n"] <- "original-positive"

ogSents <- rbind(c(ogNeg, ogPos))

```

Here is the original sentiments within the dataset counted, lets compare these to different lexicons.

```{r}
afinnNeg <- data.frame()
afinnPos <- data.frame()
# negative words
afinnNeg <- 
  cleanSA %>% 
  inner_join(get_sentiments("afinn")) %>% 
  filter(value <= -1) %>% 
  count() 
# add a column name to negative
colnames(afinnNeg)[colnames(afinnNeg) == "n"] <- "afinn-negative"
# positive words
afinnPos <-
  cleanSA %>%
  inner_join(get_sentiments("afinn")) %>%
  filter(value >= 1) %>%
  count()
# add a column name to pos
colnames(afinnPos)[colnames(afinnPos) == "n"] <- "afinn-positive"

afinnSents <- rbind(c(afinnNeg, afinnPos))
```

Now that there is a positive and negative count for afinn and the original sentiments within the dataset, lets plot them and see the differences.

```{r}
# combine the datasets
cBoundboi <- rbind(ogSents, afinnSents)

row.names(cBoundboi) <- c("original", "afinn")
colnames(cBoundboi) <- c("negative","positive")

barplot(cBoundboi,
        main = "Differences in sentiments",
        sub = "original v afinn",
        xlab = "Sentiment",
        ylab = "Observations",
        col = c("yellow", myFill)
        )
legend("topright",
       c("original","afinn"),
       fill = c("yellow", myFill)
       )
```

# 5.2 - Comparing lexicon dictionaries

This section will look at the differences with the 3 dictionaries explored within this report.

```{r}
# combine both of the dictionaries
allSA <- bind_rows(cleanSA %>% 
      inner_join(get_sentiments("afinn")) %>%
        mutate(method = "Afinn Sentiments."), 
  cleanSA %>% 
      inner_join(get_sentiments("bing")) %>%
        mutate(method = "Bing Sentiments."), 
  cleanSA %>% 
      inner_join(get_sentiments("nrc") %>% 
        filter(sentiment %in% c("positive", "negative"))) %>%
        mutate(method = "NRC Sentiments")) %>% # using the nrc dictionary, combine all sentiments as nrc sentiments
            count(method, index=ID %% 150, sentiment) %>% # count all entries of both dictionaries, with neg, pos and overall sentiment.
  spread(sentiment, n, fill = 0) %>%
  mutate(sentiment = positive - negative)

```

Now lets plot it to get an idea of how the sentiment is per 150 lines.

```{r}
allSA %>%
  ggplot(aes(index, sentiment, fill = method)) +
  geom_col(show.legend = F) +
  facet_wrap(~method, ncol = 1, scales = "free_x") +
  labs(title = "Comparing sentiments",
  subtitle = "Between the lexicon dictionaries, bing and nrc",
  y = "Emotion", x = "Observations")

```

Whilst looking at the comparisons between the 3 lexicon dictionaries, here there are a few things which stand out. Whilst this is calculating the sentiment for 150 terms, all 3 lexicon dictionaries look similar with a few differences here and there.

This could be a good or a bad thing when looking at sentiment analysis and using these lexicons, but with there being multiple choices it is good to see how they differ


# 6 - References
## 6.1 - Books
Silge, J. and Robinson, D., 2017. Text mining with R: A tidy approach. Available at: https://www.tidytextmining.com (Accessed 05/12/2020)

Winters, R. (2021), Practical Prediction Analysis, Available at: https://www.oreilly.com/library/view/practical-predictive-analytics/9781785886188/ba5bd5c5-31d7-4502-a626-fe5aa1194ca9.xhtml (Accessed 23/01/2021)

## 6.2 - Web
Silge, J. (2018), The game is afoot! Topic modeling of Sherlock Holmes stories, Available at: https://juliasilge.com/blog/sherlock-holmes-stm/ (Accessed 24/01/2021)

## 6.3 - Packages

Emil Hvitfeldt (2020). textdata: Download and Load Various Text Datasets. R package version 0.4.1. https://CRAN.R-project.org/package=textdata

Erich Neuwirth (2014). RColorBrewer: ColorBrewer Palettes. R package version 1.1-2. https://CRAN.R-project.org/package=RColorBrewer

H. Wickham. ggplot2: Elegant Graphics for Data Analysis. Springer-Verlag New York, 2016.

Hadley Wickham (2020). tidyr: Tidy Messy Data. R package version 1.1.2. https://CRAN.R-project.org/package=tidyr

Hadley Wickham, Romain François, Lionel Henry and Kirill Müller (2021). dplyr: A Grammar of Data Manipulation. R package version 1.0.3. https://CRAN.R-project.org/package=dplyr

Ian Fellows (2018). wordcloud: Word Clouds. R package version 2.6. https://CRAN.R-project.org/package=wordcloud

Ingo Feinerer and Kurt Hornik (2020). tm: Text Mining Package. R package version 0.7-8. https://CRAN.R-project.org/package=tm

Milan Bouchet-Valat (2020). SnowballC: Snowball Stemmers Based on the C 'libstemmer' UTF-8 Library. R package version 0.7.0. https://CRAN.R-project.org/package=SnowballC

RStudio Team (2020). RStudio: Integrated Development for R. RStudio, PBC, Boston, MA URL http://www.rstudio.com/.

Silge J, Robinson D (2016). “tidytext: Text Mining and Analysis Using Tidy Data Principles in R.” _JOSS_, *1*(3). doi: 10.21105/joss.00037 (URL: https://doi.org/10.21105/joss.00037), <URL: http://dx.doi.org/10.21105/joss.00037>.

## 6.4 - Software

Windows 10 - 20H2

Ubuntu-20.04-LTS

R - 4.0.2

RStudio - version 1.3.1093