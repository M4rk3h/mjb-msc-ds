knitr::opts_chunk$set(echo = FALSE)
# install.packages(c("dplyr","ggplot2","RColorBrewer","SnowballC","textdata","tidyr","tidytext","tm","topicmodels","wordcloud"))
library(dplyr)
library(ggplot2)
library(RColorBrewer)
library(reshape2)
library(SnowballC)
library(textdata)
library(tidyr)
library(tidytext)
library(tm)
library(topicmodels)
library(wordcloud)
#Read in the data
Data <- read.csv("airline_coursework(1).csv", stringsAsFactors = FALSE)
#summary(Data)
Data %>%
head(n = 5)
#class(Data)
#str(Data)
# check unique as unlist.
Data$airline %>%
unlist() %>%
unique()
# count the na values
Data %>%
is.na() %>%
sum()
# drop na values
Data <- drop_na(Data) # drop na values
# count again as sanity check.
Data %>%
is.na() %>%
sum()
# list unique again.
Data$airline %>%
unlist() %>%
unique()
# create a corpus of the text col
dcText <- Corpus(VectorSource(Data$text))
getTransformations()
# change the content to lowercase
dcText = tm_map(dcText, content_transformer(tolower))
# remove numbers
dcText = tm_map(dcText, removeNumbers)
# remove punctuation
dcText = tm_map(dcText, removePunctuation)
# remove stopwords from the english dictionary
dcText = tm_map(dcText, removeWords, (stopwords("english")))
# dcText = tm_map(dcText, stemDocument)
dcText = tm_map(dcText, stripWhitespace)
toSpace <-
content_transformer(function (x , pattern ) gsub(pattern, " ", x))
tText <- tm_map(dcText, toSpace, "/")
tText <- tm_map(dcText, toSpace, "@")
tText <- tm_map(dcText, toSpace, "\\|")
myDTM <- TermDocumentMatrix(tText)
m <- as.matrix(myDTM)
v <- sort(rowSums(m),decreasing=TRUE)
d <- data.frame(word = names(v),freq=v)
head(d, 10)
theme_set(theme_minimal())
myFill = '#E7298A'
myColour = '#000000'
barplot(d[1:10,]$freq,
las = 2,
names.arg = d[1:10,]$word,
col = myFill, main ="Top 10 words",
xlab = "Frequency",
ylab = "Word",
horiz = TRUE)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=F, rot.per=0.25,
colors=brewer.pal(4, "Dark2"))
# create my custom words list
custom_stop_words <- bind_rows(tibble(word = c("united","flight","virginamerica"),lexicon = c("custom")), stop_words)
d <- d %>% # update d with the removal of new custom stop words
anti_join(custom_stop_words)
barplot(d[1:10,]$freq,
las = 2,
names.arg = d[1:10,]$word,
col = myFill, main ="Top 10 words",
xlab = "Frequency",
ylab = "Word",
horiz = TRUE)
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq = 1,
max.words=100, random.order=F, rot.per=0.25,
colors=brewer.pal(4, "Dark2"))
# get a copy of the data for Sentiment Analysis section
saData <- Data
saData %>%
head(n = 5)
saData <- saData %>%
unnest_tokens(word, text)
saData %>%
head(n = 5)
saData <- saData %>%
anti_join(stop_words) # 27411
saData %>%
head(n = 5)
saData %>%
count(word, sort = TRUE) %>%
head(n=5)
# update custom stop words
custom_stop_words <-
bind_rows(tibble(word = c("united","flight","virginamerica", "t.co","http", "2"),lexicon = c("custom")), stop_words)
# remove custom stop words pt 2
saData <- saData %>%
anti_join(custom_stop_words) # 27411
# sanity check to see how it looks now.
saData %>%
count(word, sort = TRUE) %>%
head(n=5)
saData %>%
count(word, sort = TRUE) %>%
filter(n > 125) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(word, n)) +
geom_col(fill = myFill, color = myColour) +
labs(title = "Highest recoccouring word",
subtitle = "With custom stop words removed",
y = "Observations", x = "Terms") +
coord_flip()
cleanSA <- saData %>%
anti_join(stop_words, by=c("word"="word"))
# count again for a sanity check
saData %>%
count(word, sort = TRUE) %>%
head(n = 5)
bingSA <- cleanSA %>%
inner_join(get_sentiments("bing")) %>%
count(word, sort = T)
bingSA %>%
head(n = 5)
nrcSA <- cleanSA %>%
inner_join(get_sentiments("nrc")) %>%
count(word, sort = T)
nrcSA %>%
head(n = 5)
# create an index for cleanSA
cleanSA$ID <- seq.int(nrow(cleanSA))
# combine both of the dictionaries
bingNRC <- bind_rows(cleanSA %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing Sentiments."), # using the bing dictionary, combine all sentiments as bing sentiments
cleanSA %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC Sentiments")) %>% # using the nrc dictionary, combine all sentiments as nrc sentiments
count(method, index=ID %% 150, sentiment) %>% # count all entries of both dictionaries, with neg, pos and overall sentiment.
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
bingNRC %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = F) +
facet_wrap(~method, ncol = 1, scales = "free_x") +
labs(title = "Comparing sentiments",
subtitle = "Between the lexicon dictionaries, bing and nrc",
y = "Emotion", x = "Observations")
# Bing
get_sentiments("bing") %>%
head(n = 5)
# nrc
get_sentiments("nrc") %>%
head(n = 5)
# afinn
get_sentiments("afinn") %>% head(n = 5)
afinnSA <- cleanSA %>%
inner_join(get_sentiments('afinn')) %>%
count(word, value, sort = TRUE)
afinnSA[1:10,]
afinnSA %>%
filter(n > 40) %>%
mutate(word = reorder(word, n)) %>%
ggplot(aes(n, word, color=word)) +
geom_col(fill='#ffffff') +
labs(title = "Highest recoccouring word",
subtitle = "Within the lexicon dictionary, afinn",
y = "Terms", x = "Observations")
topicD <- cleanSA
topicD %>%
count(word, sort = T) %>%
head(n = 5)
# change to corpus using TM packages
myCorp <- Corpus(VectorSource(topicD$word))
# now convert to DocumentTermMatrix and clean it
myDTM <- DocumentTermMatrix(myCorp, control = list())
# If the DTM has errors when creating an LDA,
# find all empty rows and remove them.
#sum by raw each raw of the table
raw.sum = apply(myDTM,1,FUN=sum)
# remove the
myDTM = myDTM[raw.sum!=0,]
myLDA <- LDA(myDTM, k=2, control=list(seed=1234))
ldaTopics <- tidy(myLDA, matrix="beta")
ldaTopics
# get the top 10 terms from ldaTopics
topTerms <- ldaTopics %>%
group_by(topic) %>%
top_n(10) %>%
ungroup() %>%
mutate(term = reorder(term, beta)) %>%  # count by term, then data.
arrange(topic, -beta)
# not plot the top terms
topTerms %>%
ggplot(aes(term, beta, fill = factor(topic), colour=myColour)) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
labs(title = "Top ten words in each topic",
y = "Beta", x = "Term")+
coord_flip()
beta_spread <- ldaTopics %>%
mutate(topic = paste0("topic", topic)) %>%
spread(topic, beta) %>%
filter(topic1 > .001 | topic2 > .001) %>%
mutate(log_ratio = log2(topic2 / topic1))
beta_spread
beta_top_terms <- beta_spread %>%
top_n(10, log_ratio)
beta_top_terms %>%
ggplot(aes(term, log_ratio, fill="#E7298A")) +
geom_col(show.legend = FALSE) +
coord_flip() +
labs(title = "Words with biggest differences in the 2 topics",
y = "Terms", x = "Log Ratio")
# get a copy of the data
ogSA <- Data
# create 2 data frames
ogPos <- data.frame()
ogNeg <- data.frame()
# filter and count neg
ogNeg <-
ogSA %>%
filter(ogSA$sentiment == "negative") %>%
count()
# add colnames
colnames(ogNeg)[colnames(ogNeg) == "n"] <- "original-negative"
# filter and count pos
ogPos <-
ogSA %>%
filter(ogSA$sentiment == "positive") %>%
count()
# add colnames
colnames(ogPos)[colnames(ogPos) == "n"] <- "original-positive"
ogSents <- rbind(c(ogNeg, ogPos))
afinnNeg <- data.frame()
afinnPos <- data.frame()
# negative words
afinnNeg <-
cleanSA %>%
inner_join(get_sentiments("afinn")) %>%
filter(value <= -1) %>%
count()
# add a column name to negative
colnames(afinnNeg)[colnames(afinnNeg) == "n"] <- "afinn-negative"
# positive words
afinnPos <-
cleanSA %>%
inner_join(get_sentiments("afinn")) %>%
filter(value >= 1) %>%
count()
# add a column name to pos
colnames(afinnPos)[colnames(afinnPos) == "n"] <- "afinn-positive"
afinnSents <- rbind(c(afinnNeg, afinnPos))
# combine the datasets
cBoundboi <- rbind(ogSents, afinnSents)
row.names(cBoundboi) <- c("original", "afinn")
colnames(cBoundboi) <- c("negative","positive")
barplot(cBoundboi,
main = "Differences in sentiments",
sub = "original v afinn",
xlab = "Sentiment",
ylab = "Observations",
col = c("yellow", myFill)
)
legend("topright",
c("original","afinn"),
fill = c("yellow", myFill)
)
# combine both of the dictionaries
allSA <- bind_rows(cleanSA %>%
inner_join(get_sentiments("afinn")) %>%
mutate(method = "Afinn Sentiments."),
cleanSA %>%
inner_join(get_sentiments("bing")) %>%
mutate(method = "Bing Sentiments."),
cleanSA %>%
inner_join(get_sentiments("nrc") %>%
filter(sentiment %in% c("positive", "negative"))) %>%
mutate(method = "NRC Sentiments")) %>% # using the nrc dictionary, combine all sentiments as nrc sentiments
count(method, index=ID %% 150, sentiment) %>% # count all entries of both dictionaries, with neg, pos and overall sentiment.
spread(sentiment, n, fill = 0) %>%
mutate(sentiment = positive - negative)
allSA %>%
ggplot(aes(index, sentiment, fill = method)) +
geom_col(show.legend = F) +
facet_wrap(~method, ncol = 1, scales = "free_x") +
labs(title = "Comparing sentiments",
subtitle = "Between the lexicon dictionaries, bing and nrc",
y = "Emotion", x = "Observations")
# check unique as unlist.
Data$airline %>%
unlist() %>%
unique()
# count the na values
Data %>%
is.na() %>%
sum()
# drop na values
Data <- drop_na(Data) # drop na values
# count again as sanity check.
Data %>%
is.na() %>%
sum()
# list unique again.
Data$airline %>%
unlist() %>%
unique()
ogSents
afinnSents
