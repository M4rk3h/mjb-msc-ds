---
title: "Bag of Words Example - Student"
author: "Penny Holborn"
date: "1 December 2020"
output: html_document
---

Example taken from http://www.sthda.com/english/wiki/text-mining-and-word-cloud-fundamentals-in-r-5-simple-steps-you-should-know

```{r}
install.packages("tm")
install.packages("SnowballC")
install.packages("wordcloud")
install.packages("RColorBrewer")
install.packages("ggplot2")
```

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```


```{r}
library(tm) #Text mining
library(SnowballC) #Required for stemming
library(RColorBrewer) #Required for color palettes
library(ggplot2) #Required for plotting
library(wordcloud) #Required for Word Cloud
```


```{r}
# Read the text file from internet
filePath <- "http://www.sthda.com/sthda/RDoc/example-files/martin-luther-king-i-have-a-dream-speech.txt"

```

The text is loaded using Corpus() function from text mining (tm) package. Corpus is a list of a document (in our case, we only have one document).


Load the data as a corpus
```{r}
docs <- Corpus(VectorSource(text))
inspect(docs)
getTransformations()
```

Transformation is performed using tm_map() function to replace, for example, special characters from the text. Replacing "/", "@" and "|" with space:

```{r}
# create a function to remove something
toSpace <- content_transformer(function (x , pattern) gsub(pattern, " ", x))
# use tm_map to remove these special characters.
docs <- tm_map(docs, toSpace, "/")
docs <- tm_map(docs, toSpace, "@")
docs <- tm_map(docs, toSpace, "\\|")
```

The tm_map() function is used to remove unnecessary white space, to convert the text to lower case, to remove common stopwords like 'the', "we".
The R code below can be used to clean your text :

```{r}
# convert to lower
docs <- tm_map(docs, content_transformer(tolower))
# remove numbers
docs <- tm_map(docs, removeNumbers)
# remove english common stopwords
docs <- tm_map(docs, removeWords, stopwords("english"))
# remove punctuation
docs <- tm_map(docs, removePunctuation)
# eliminate extra white space
docs <- tm_map(docs, stripWhitespace)
# text stemming
docs <- tm_map(docs, stemDocument)
# specify your stopwords as a character vector
# custom stopwords
docs <- tm_map(docs, removeWords, c("blabla1","blabla2"))
inspect(docs)
```

The function TermDocumentMatrix() from text mining package can be used as follow :

```{r}
dtm <- TermDocumentMatrix(docs)
m <- as.matrix(dtm)
v <- sort(rowSums(m), decreasing = T)
d <- data.frame(word = names(v), freq=v)
head(d, 10)
```

Explore frequent terms and their associations
You can have a look at the frequent terms in the term-document matrix as follow. In the example below we want to find words that occur at least four times :

```{r}
findFreqTerms(dtm, lowfreq = 4)
```


The frequency of the first 10 frequent words are plotted :
```{r}
barplot(d[1:10,]$freq, las = 2, names.arg = d[1:10,]$word,
        col = "lightblue", main = "Most frequent words",
        ylab = "word frequencies")
```

You can analyse the association between frequent terms (i.e., terms which correlate) using findAssocs() function. The R code below identifies which words are associated with "freedom" in I have a dream speech :

```{r}
findAssocs(dtm, terms = "freedom", corlimit = 0.3)
```

The importance of words can be illustrated as a word cloud as follow :
```{r}
set.seed(1234)
wordcloud(words = d$word, freq = d$freq, min.freq=1,
          max.words = 200, random.order = F, rot.per = 0.35,
          colors = brewer.pal(8, "Dark2"))
```