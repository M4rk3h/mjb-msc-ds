{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iclhxZ6vImZP"
   },
   "source": [
    "# MS4S10-Assessment 2\n",
    "## Mark Baber - 17076749\n",
    "\n",
    "This report will go over a few algorithms which are great for machine learning, these algorithms are as follows:\n",
    "- Gradient Descent\n",
    "- Logistic Regression (as a neural network)\n",
    "- MNIST"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KtIbW0uKIqLn"
   },
   "source": [
    "### Task 1 - Gradient Descent (15%)\n",
    "\n",
    "Apply the gradient descent algorithm to find the minimum of the following function of four variables.\n",
    "\\begin{equation}\n",
    "f(x,y,z,w) = \\frac{1}{4} (2-x)^2 + (3y-5)^4 + e^{2z^{4}+w^2}\n",
    "\\end{equation}\n",
    "\n",
    "Comment on the Results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GLmIGljxafCR",
    "outputId": "ac4fc5c7-c60e-4353-c4a6-8c59d993298d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 43 25 13 inf\n",
      "32.56 9.61 -inf 13 inf\n",
      "31.184800000000003 3.2659000000000002 -inf 13 inf\n",
      "29.871484000000002 2.060521 -inf 13 inf\n",
      "28.617267220000002 1.8314989899999998 -inf 13 inf\n",
      "27.419490195100003 1.7879848081 -inf 13 inf\n",
      "26.2756131363205 1.779717113539 -inf 13 inf\n",
      "25.18321054518608 1.77814625157241 -inf 13 inf\n",
      "24.139966070652704 1.777847787798758 -inf 13 inf\n",
      "23.143667597473332 1.777791079681764 -inf 13 inf\n",
      "22.19220255558703 1.777780305139535 -inf 13 inf\n",
      "21.283553440585614 1.7777782579765116 -inf 13 inf\n",
      "20.41579353575926 1.7777778690155372 -inf 13 inf\n",
      "19.587082826650093 1.777777795112952 -inf 13 inf\n",
      "18.79566409945084 1.7777777810714608 -inf 13 inf\n",
      "18.03985921497555 1.7777777784035775 -inf 13 inf\n",
      "17.31806555030165 1.7777777778966797 -inf 13 inf\n",
      "16.628752600538075 1.7777777778003692 -inf 13 inf\n",
      "15.970458733513862 1.7777777777820702 -inf 13 inf\n",
      "15.341788090505737 1.7777777777785932 -inf 13 inf\n",
      "14.741407626432979 1.7777777777779327 -inf 13 inf\n",
      "14.168044283243495 1.7777777777778074 -inf 13 inf\n",
      "13.620482290497538 1.7777777777777832 -inf 13 inf\n",
      "13.097560587425148 1.7777777777777788 -inf 13 inf\n",
      "12.598170360991016 1.777777777777778 -inf 13 inf\n",
      "12.12125269474642 1.7777777777777777 -inf 13 inf\n",
      "11.665796323482832 1.7777777777777777 -inf 13 inf\n",
      "11.230835488926104 1.7777777777777777 -inf 13 inf\n",
      "10.81544789192443 1.7777777777777777 -inf 13 inf\n",
      "10.41875273678783 1.7777777777777777 -inf 13 inf\n",
      "10.039908863632379 1.7777777777777777 -inf 13 inf\n",
      "9.678112964768921 1.7777777777777777 -inf 13 inf\n",
      "9.33259788135432 1.7777777777777777 -inf 13 inf\n",
      "9.002630976693375 1.7777777777777777 -inf 13 inf\n",
      "8.687512582742173 1.7777777777777777 -inf 13 inf\n",
      "8.386574516518776 1.7777777777777777 -inf 13 inf\n",
      "8.099178663275431 1.7777777777777777 -inf 13 inf\n",
      "7.824715623428037 1.7777777777777777 -inf 13 inf\n",
      "7.562603420373775 1.7777777777777777 -inf 13 inf\n",
      "7.3122862664569555 1.7777777777777777 -inf 13 inf\n",
      "7.073233384466392 1.7777777777777777 -inf 13 inf\n",
      "6.8449378821654046 1.7777777777777777 -inf 13 inf\n",
      "6.626915677467961 1.7777777777777777 -inf 13 inf\n",
      "6.418704471981903 1.7777777777777777 -inf 13 inf\n",
      "6.219862770742718 1.7777777777777777 -inf 13 inf\n",
      "6.029968946059295 1.7777777777777777 -inf 13 inf\n",
      "5.848620343486627 1.7777777777777777 -inf 13 inf\n",
      "5.675432428029729 1.7777777777777777 -inf 13 inf\n",
      "5.510037968768391 1.7777777777777777 -inf 13 inf\n",
      "5.352086260173814 1.7777777777777777 -inf 13 inf\n",
      "5.2012423784659925 1.7777777777777777 -inf 13 inf\n",
      "5.057186471435023 1.7777777777777777 -inf 13 inf\n",
      "4.919613080220447 1.7777777777777777 -inf 13 inf\n",
      "4.788230491610527 1.7777777777777777 -inf 13 inf\n",
      "4.662760119488054 1.7777777777777777 -inf 13 inf\n",
      "4.542935914111092 1.7777777777777777 -inf 13 inf\n",
      "4.428503797976092 1.7777777777777777 -inf 13 inf\n",
      "4.319221127067168 1.7777777777777777 -inf 13 inf\n",
      "4.214856176349145 1.7777777777777777 -inf 13 inf\n",
      "4.115187648413434 1.7777777777777777 -inf 13 inf\n",
      "4.020004204234829 1.7777777777777777 -inf 13 inf\n",
      "3.929104015044262 1.7777777777777777 -inf 13 inf\n",
      "3.84229433436727 1.7777777777777777 -inf 13 inf\n",
      "3.759391089320743 1.7777777777777777 -inf 13 inf\n",
      "3.6802184903013098 1.7777777777777777 -inf 13 inf\n",
      "3.604608658237751 1.7777777777777777 -inf 13 inf\n",
      "3.532401268617052 1.7777777777777777 -inf 13 inf\n",
      "3.4634432115292846 1.7777777777777777 -inf 13 inf\n",
      "3.397588267010467 1.7777777777777777 -inf 13 inf\n",
      "3.3346967949949957 1.7777777777777777 -inf 13 inf\n",
      "3.274635439220221 1.7777777777777777 -inf 13 inf\n",
      "3.217276844455311 1.7777777777777777 -inf 13 inf\n",
      "3.162499386454822 1.7777777777777777 -inf 13 inf\n",
      "3.110186914064355 1.7777777777777777 -inf 13 inf\n",
      "74\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-1-4bd63eb68119>:20: RuntimeWarning: overflow encountered in exp\n",
      "  return 1/4*(2-x)**2 + (3*y-5)**4 + np.exp(2*z**4+w**2)\n",
      "<ipython-input-1-4bd63eb68119>:29: RuntimeWarning: overflow encountered in exp\n",
      "  return np.exp(2*z**4+w**2)\n"
     ]
    }
   ],
   "source": [
    "# start with numpy as np\n",
    "import numpy as np\n",
    "\n",
    "# condition when stop\n",
    "# tolerance = 0.01\n",
    "tolerance = 0.05\n",
    "\n",
    "# size of the step\n",
    "# step_size = 0.01\n",
    "step_size = 0.09\n",
    "\n",
    "# starting point \n",
    "x = 34\n",
    "y = 43\n",
    "z = 25\n",
    "w = 13\n",
    "\n",
    "# definition of the function\n",
    "def fun(x,y,z,w):\n",
    "    return 1/4*(2-x)**2 + (3*y-5)**4 + np.exp(2*z**4+w**2)\n",
    "# first partial derivative\n",
    "def der1(x):\n",
    "    return 2*1/4*(2-x)*-1\n",
    "# second partial derivative\n",
    "def der2(y):\n",
    "    return 3*(3*y-5)-1\n",
    "# third partial derivative\n",
    "def der3(z, w):\n",
    "    return np.exp(2*z**4+w**2)\n",
    "\n",
    "# function to carry out the step\n",
    "def step(z, grad, step_size):\n",
    "    return (z - step_size * grad)\n",
    "\n",
    "# counter for number of steps\n",
    "i = 0\n",
    "\n",
    "while True:\n",
    "    i += 1\n",
    "    print(x, y, z, w, fun(x,y,z,w))\n",
    "    \n",
    "    grad_x = der1(x)\n",
    "    grad_y = der2(y)\n",
    "    grad_z = der3(z, w)\n",
    "    \n",
    "    next_x = step(x, grad_x, step_size)\n",
    "    next_y = step(y, grad_y, step_size)\n",
    "    next_z = step(z, grad_z, step_size)\n",
    "    #next_w = step(w, grad_w, step_size)\n",
    "    \n",
    "    if (np.sqrt(\n",
    "        (x - next_x)**2 + \n",
    "        (y - next_y)**2)  < tolerance):\n",
    "        break # breaks out of the while true loop\n",
    "                #(z - next_z)**2 + \n",
    "                #(w - next_w)**2) \n",
    "                \n",
    "        \n",
    "    x, y, z = next_x, next_y, next_z #, next_w\n",
    "print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FJiANRQ-S05n"
   },
   "source": [
    "Gradient Descent is an optimization algorithm which is widely used to optimize a neural network (S.Ruder 2016). Above is an example of using a Gradient descent algorithm to find the mininimum of the given function.\n",
    "<br>\n",
    "When running the above code with these variables:\n",
    "- x = 39\n",
    "- y = 26\n",
    "- z = 48\n",
    "- w = 6\n",
    "\n",
    "It took 584 to find the minimum of the function with the last 2 values being as follows:\n",
    "- 4.0009430038071985 1.7777777777777788 -inf 6 inf\n",
    "- 3.9909382887881626 1.7777777777777788 -inf 6 inf\n",
    "\n",
    "Which shows a tiny difference between the two, this could also be due to the step size being small also with a value of 0.01. Whilst this could become more accurate, it is important to find an accurate and well performed algorithm for the task.\n",
    "\n",
    "If the values were to change for a few of these variables;\n",
    "- x = 34\n",
    "- y = 43\n",
    "- z = 25\n",
    "- w = 13\n",
    "\n",
    "Here we can see it took only 74 counts of i to get the minimum of the function, with a bigger step size and tolerance. This has improved performance but is also not as accurate. \n",
    "\n",
    "src = https://www.ibm.com/cloud/learn/gradient-descent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bi9hf-nIadZu"
   },
   "source": [
    "### Task 2 – Logistic Regression as a Neural Network with another activation function (45%)\n",
    "\n",
    "Write a report on how to consider Logistic Regression as a Neural Network with one single neuronusing\n",
    "\n",
    "\\begin{align}\n",
    "tanh z = \\frac{{e^z-e^{-z}}}{{e^z+e^{-z}}}\n",
    "\\end{align}\n",
    "\n",
    "as the activation function.\n",
    "<br><br>\n",
    "Observe that the range of the hyperbolic tangent is the interval (−1;1). Youwillneed to rescale the activation function and/or the cross-entropy loss functionas required.\n",
    "<br>\n",
    "Focus in particular on:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "h-yv-HSAolwW"
   },
   "source": [
    "#### 2.1 - The calculation of the derivative of theactivation functionand how to express the derivative using the hyperbolic tangent itself.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qtoVhdS1ozxO"
   },
   "source": [
    "#### 2.2 - Discuss the behaviour of the cross-entropy loss function. Use matplotlib to draw appropriate graphs and comment on the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHtmNOAqozkR"
   },
   "source": [
    "#### 2.3 Evaluate \n",
    "\n",
    "\\begin{align}\n",
    "\\frac {\\delta L^{i}}{\\delta wk} \\text{ and} \\frac {\\delta L^{i}}{\\delta b }\n",
    "\\end{align}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUCTiVrsozU5"
   },
   "source": [
    "### Task 3 – MNIST (30%)\n",
    "Consider the example of the MNIST digits datasets we discussed in class. Try at least 4 different Neural Networks architectures (by changing the number of neurons and the number of layers) and compare their performances using:\n",
    "\n",
    "- At least two activation functions (sigmoid, relu, ...);\n",
    "- At least four different step sizes;\n",
    "- At least three batch sizes;\n",
    "- At least three optimisation algorithms (remember to use at least three different values for the momentum when using gradient descent);\n",
    "- At least two parameter initialisations.\n",
    "<br><br>\n",
    "Draw appropriate graphs and comment on them to support your conclusions.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g3vGXLqqX3P4"
   },
   "source": [
    "#### 3.1 MNIST dataset with atleast 2 activation functions (sigmoid, relu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "id": "UHUyIzjrYEZ2"
   },
   "outputs": [],
   "source": [
    "# 3.1 - MNIST\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.datasets import make_moons\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras import models\n",
    "from keras import layers \n",
    "from keras.datasets import mnist, reuters\n",
    "from keras.utils import to_categorical\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "J8ia56dtYNbh",
    "outputId": "33faf5ec-6b01-4ec9-8b3e-5436e744167a"
   },
   "outputs": [],
   "source": [
    "# Here we will import the dataset\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()\n",
    "# 'normalize' the dataset\n",
    "X_train = tf.keras.utils.normalize(X_train, axis=1)\n",
    "X_test = tf.keras.utils.normalize(X_test, axis=1)\n",
    "# this makes the data easier to learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train: (60000, 28, 28)\n",
      "y_train: (60000,)\n",
      "X_test:  (10000, 28, 28)\n",
      "y_test:  (10000,)\n"
     ]
    }
   ],
   "source": [
    "# look at the shape of the dataset\n",
    "print('X_train: ' + str(X_train.shape))\n",
    "print('y_train: ' + str(y_train.shape))\n",
    "print('X_test:  '  + str(X_test.shape))\n",
    "print('y_test:  '  + str(y_test.shape))\n",
    "# there are 60,000 images within the dataset\n",
    "# which are 28 * 28 pixels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x129ee9ed760>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# look at the first entry as binary colour map\n",
    "plt.imshow(X_train[0], cmap = plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYHFhDX5ascK"
   },
   "source": [
    "Building A Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 913
    },
    "id": "73DOckEJap8R",
    "outputId": "d4ddc439-9202-4d33-bb97-3a5d40002b68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Architecture complete\n",
      "Optimizer complete\n"
     ]
    }
   ],
   "source": [
    "## 1 ## Architecture\n",
    "network = models.Sequential()\n",
    "# flatten the dataset as an input layer\n",
    "network.add(tf.keras.layers.Flatten())\n",
    "# hidden layer 1\n",
    "network.add(layers.Dense(20, activation='relu'))\n",
    "# hidden layer 2\n",
    "network.add(layers.Dense(10, activation='softmax'))\n",
    "##\n",
    "print('Architecture complete')\n",
    "## 2 ## Optimizer and Error function\n",
    "network.compile(optimizer='adam', # (adam, gradient descent)\n",
    "                loss='binary_crossentropy', # degree of error\n",
    "                metrics=['accuracy'])\n",
    "print('Optimizer complete')\n",
    "## 3 ## Train\n",
    "# network.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GlhS2iQOaPfG"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GomGX_MlUhYz"
   },
   "source": [
    "### References\n",
    "\n",
    "Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv preprint arXiv:1609.04747.\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MS4S10-Assessment 2.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
