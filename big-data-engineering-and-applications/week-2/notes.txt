Here is the VM for the coursework:
	TO DO: SCREENSHOT HERE

	created a user as mark to set it up (as the other 2 had a username of mark)
	created a new user from the terminal 
	# adduser hadoop-cw
	made the user a sudoer
	# usermod -aG sudo hadoop-cw


After installing Ubuntu Desktop, the first thing I did was to update the VM, this can be done in the terminal with:
	sudo apt update && sudo apt upgrade -y

Then I installed openssh so that I could 'remote' into my server from my local pc without the gui.
	# install ssh & check the status
	sudo apt install openssh-server && sudo systemctl status ssh

Whilst also checking the hostname to make sure it is 'worker-cw'
	sudo hostname 
	# if not 'worker-cw' can change with 'sudo nano /etc/hostname'

The next step is to assign the hosts for the hadoop Cluster
	sudo nano /etc/hosts

	# add vm ip and bdea-master
	127.0.0.1		localhost
	#127.0.1.1		bdea-xyz (this would be VirtualMachine hostname)

	192.168.1.170	bdea-main
	192.168.1.171	bdea-worker
	192.168.1.172	hadoop-cw

# install JAVA
sudo apt install openjdk-8-jdk

# add to s/bin path variables to bashrc
	sudo nano ~/.bashrc

	# JAVA
	export JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre

	source ~/.bashrc

# download hadoop
wget https://archive.apache.org/dist/hadoop/common/hadoop-3.2.2/hadoop-3.2.2.tar.gz

# extract hadoop
tar -xvf hadoop-3.2.2.tar.gz

# remove archive
rm hadoop-3.2.2.tar.gz

# move and rename hadoop
sudo mv hadoop-3.2.2 /usr/local/hadoop

# add to s/bin path variables
sudo nano ~/.bashrc
# HADOOP
export PATH=$PATH:/usr/local/hadoop/bin:/usr/local/hadoop/sbin
export CONF=/usr/local/hadoop/etc/hadoop

# source bash
source ~/.bashrc

# CLONE VM HERE
# reset the mac address, leave it on dhcp

# can change to static ip below
sudo nano /etc/netplan/01-network-manager-all.yaml

network:
  version: 2
  renderer: networkd
  ethernets:
    enp0s3:
      dhcp4: no
      addresses:
        - 192.168.1.171/24 # ip which you know if free
      gateway4: 192.168.1.254 # your router ip here
      nameservers:
          addresses: [8.8.8.8, 1.1.1.1]

sudo netplan apply

# generate ssh keys
ssh-keygen -t rsa

from main node, copy ssh-keys to other workers
# copy master key (public) to worker
ssh-copy-id hadoop-cw@hadoop-main
ssh-copy-id hadoop-cw@hadoop-worker
ssh-copy-id hadoop-cw@worker-cw

## EDIT the 4 files
1 - hadoop-env.sh
# sudo nano $CONF/hadoop-env.sh
add this line
JAVA_HOME=/usr/lib/jvm/java-8-openjdk-amd64/jre

2 - core-site.xml
# sudo nano $CONF/core-site.xml

<configuration>
	<property>
		<name>fs.defaultFS</name>
		<value>hdfs://hadoop-main:9000</value>
	</property>
</configuration>

3 - hdfs-site.xml
# sudo nano $CONF/hdfs-site.xml

<configuration>
        <property>
                <name>dfs.namenode.name.dir</name>
                <value>/usr/local/hadoop/data/nameNode</value>
        </property>
        <property>
                <name>dfs.datanode.name.dir</name>
                <value>/usr/local/hadoop/data/dataNode</value>
        </property>
        <property>
                <name>dfs.replication</name>
                <value>2</value>
        </property>
</configuration>

4 - workers
# sudo nano $CONF/workers

# after configuring the 4 files on masters, copy to worker node
scp $CONF/* hadoop-cw@bdea-worker:/usr/local/hadoop/etc/hadoop
scp $CONF/* hadoop-cw@worker-cw:/usr/local/hadoop/etc/hadoop

#### Mapreduce on Hadoop Cluster #### 
2.1 - Configuration on hadoop

2.1.1 Worker nodes
make sure the yarn-site.xml within the workers are setup

<configuration>
        <property>
                <name>yarn.resourcemanager.hostname</name>
                <value>hadoop-main</value>
        </property>
</configuration>



<configuration> 
  <property> 
    <name>yarn.resourcemanager.hostname</name>
    <value>hadoop-main</value> 
  </property> 
  <property>
    <name>fs.default.name</name>
    <value>hdfs://localhost:9000</value> 
  </property> 
</configuration>

2.1.2 - Master node
make sure the yarn-site.xml is empty on main/master

# add more extries to bdea-master .bashrc
# HADOOP MAPPING
export HADOOP_HOME=/usr/local/hadoop
export HADOOP_COMMON_HOME=$HADOOP_HOME
export HADOOP_CONF_DIR=$HADOOP_HOME/etc/hadoop
export HADOOP_HDFS_HOME=$HADOOP_HOME
export HADOOP_MAPPED_HOME=$HADOOP_HOME

source ~/.bashrc 
	then format the main server node (bdea-main) with
hadoop namenode -format

2.1.3 - SSH Error
This had a permissions error, which required some troubleshooting. I started by copying my sshkey from the main node,
to all of the 

2.2 execute a demo mapreduce
	start hadoop on master - start-dfs.sh
	type - jps
	start yarn - start-yarn.sh
	list yarn nodes - yarn node -list
	
	test a demo map reduce task
	yarn jar /usr/local/hadoop/share/hadoop/mapreduce/hadoop-mapreduce-examples-3.2.2.jar pi 16 50000

2.3 - A word count mapreduce task locally with python

2.4 - Word count mapreduce on hadoop
copy the file to the server (can paste it from terminal or download from bb)
copy the file from the server locally, to hadoop file system
	make a directory
		hadoop fs -mkdir wordcount_demo
	copy the file to a demo textfile
		hadoop fs -copyFromLocal test.txt wordcount_demo/textfile.txt
	check file has been moved
		hadoop fs -ls wordcount_demo/textfile.txt

execute the yarn job
	yarn jar /usr/local/hadoop/share/hadoop/tools/lib/hadoop-streaming-3.2.2.jar \
	-files /home/hadoop-cw/Downloads/mapper.py,/home/hadoop-cw/Downloads/reducer.py \
	-mapper  '/home/hadoop-cw/Downloads/mapper.py' \
	-reducer '/home/hadoop-cw/Downloads/reducer.py' \
	-input 'wordcount_demo/textfile.txt' \
	-output 'wordcount_demo/output_demo1'
	
There could also be errors on the read and write for the file.